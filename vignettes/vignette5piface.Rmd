---
title: "Using a pipeline interface in your project"
author: "Michal Stolarczyk"
date: "`r Sys.Date()`"
output: BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{Using a pipeline interface in your project}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

Pipeline interface tells the pipeline submission engine (such as [`looper`](http://code.databio.org/looper/)) how to interact with your project and pipelines. It is just a `yaml` file with four possible sections:

* `protocol_mapping` - maps sample protocol (the assay type, sometimes called "library" or "library strategy") to one or more pipeline program
* `pipelines` - describes the arguments and resources required by each pipeline
* `collator_mapping` - maps project protocol to one or more collator program
* `collators` - describes the arguments and resources required by each collator (project-level pipeline)

Read more about the pipeline interface concept in the `looper` documentation sections linked below:

* [How to link a project to a pipeline](http://code.databio.org/looper/linking-a-pipeline/)
* [How to write a pipeline interface](http://code.databio.org/looper/pipeline-interface/)
* [How to link to multiple pipelines](http://code.databio.org/looper/linking-multiple-pipelines/)

# Main features

Let's consider the examples below that illustrate the pipeline interface-related functionality of `BiocProject` package. 

## `bioconductor` section in the pipeline interface

The first advantage of pipeline interfce concept is the possibility to declare the data processing function in the pipeline interface itself. Since the data processing function is pipeline specific rather than project specific, it is much more convenient to place the `bioconductor` section in the pipeline interface file.

```{r echo=F,message=FALSE, collapse=TRUE, comment=" "}
library(BiocProject)
branch = "cfg2"
configFile = system.file(
    "extdata",
    paste0("example_peps-", branch),
    "example_piface",
    "project_config.yaml",
    package = "BiocProject"
)
p=pepr::Project(configFile)
.printNestedList(yaml::read_yaml(pipelineInterfacesBySample(p)[[1]][1]))
```

## Get output file paths

Pipeline outputs can be defined in a schema. As shown in the example above, pipeline interface specifies a path to a schema in a top-level `output_schema` section. 
Example of a schema defining pipeline outputs:
```{r, echo=F, message=F}
pifaceSource = pipelineInterfacesBySample(p)[[1]][1]
piface = yaml::read_yaml(pifaceSource)
.printNestedList(.readSchema(piface$sample_pipeline$output_schema, dirname(pifaceSource)))
```

### Sample-level

Pipeline interface system divides pipelines (and their outputs) into project- and sample-level. 

In order to list the outputs for a sample, or all the samples use `getOutputsBySample` method:
```{r}
getOutputsBySample(p)
getOutputsBySample(p, sampleNames=c("sample1", "sample2"))
```

### Project-level

In order to list the outputs for the project use `getProjectOutputs` method:

```{r}
getProjectOutputs(p)
```

# Use case

This functionality provides a convenient way to process the files produced by the pipeline, when used in the data processing function indicated in the `bioconductor` section of the pipeline interface file. See the example function below that demonstrates the application of the `getSampleOutputs` function.

```{r echo=FALSE, eval=TRUE, comment=""}
branch = "cfg2"
processFunction = system.file(
  "extdata",
  paste0("example_peps-", branch),
  "example_piface",
  "readData.R",
  package = "BiocProject"
)
source(processFunction)
piface = yaml::read_yaml(pipelineInterfacesBySample(p)[[1]][1])
get(piface$bioconductor$readFunName)
```
Such a link between the project and the outputs (declared in the pipeline interface) makes it possible to read and process the pipeline results with just a line of code:

```
bp = BiocProject(configFile)
```